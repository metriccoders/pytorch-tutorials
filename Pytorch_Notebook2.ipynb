{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfesq0eQTqXmOalZeb60ig"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ufyFvfdw0XYb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_in, n_h, n_out, batch_size = 10, 5, 1, 10"
      ],
      "metadata": {
        "id": "_DgqJdmP0eql"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(batch_size, n_in)"
      ],
      "metadata": {
        "id": "ItHFdW7X0okt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([[1.0], [1.2], [1.8], [2.1], [1.3], [1.9], [1.2], [1.1], [1.2], [1.3]])"
      ],
      "metadata": {
        "id": "aI8bVJTp0tN4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out), nn.Sigmoid())"
      ],
      "metadata": {
        "id": "NNIYTcro08ZL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "hq9HSLSa1Gy7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "r_x5TxsY1M0x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "  y_pred = model(x)\n",
        "  loss = criterion(y_pred, y)\n",
        "  print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXPdsqiz1Syg",
        "outputId": "16cc0628-0091-46b8-a05b-21a0c82703e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 loss: 0.44358688592910767\n",
            "epoch: 1 loss: 0.44158291816711426\n",
            "epoch: 2 loss: 0.43961915373802185\n",
            "epoch: 3 loss: 0.4376944601535797\n",
            "epoch: 4 loss: 0.43580788373947144\n",
            "epoch: 5 loss: 0.4339587092399597\n",
            "epoch: 6 loss: 0.43214601278305054\n",
            "epoch: 7 loss: 0.4303690791130066\n",
            "epoch: 8 loss: 0.4286269247531891\n",
            "epoch: 9 loss: 0.426920086145401\n",
            "epoch: 10 loss: 0.42524632811546326\n",
            "epoch: 11 loss: 0.423604816198349\n",
            "epoch: 12 loss: 0.4219951033592224\n",
            "epoch: 13 loss: 0.4204161763191223\n",
            "epoch: 14 loss: 0.4188675284385681\n",
            "epoch: 15 loss: 0.4173489511013031\n",
            "epoch: 16 loss: 0.41586270928382874\n",
            "epoch: 17 loss: 0.4144042432308197\n",
            "epoch: 18 loss: 0.4129731059074402\n",
            "epoch: 19 loss: 0.41156870126724243\n",
            "epoch: 20 loss: 0.4101901650428772\n",
            "epoch: 21 loss: 0.4088371694087982\n",
            "epoch: 22 loss: 0.4075089395046234\n",
            "epoch: 23 loss: 0.4062051773071289\n",
            "epoch: 24 loss: 0.40492504835128784\n",
            "epoch: 25 loss: 0.40366822481155396\n",
            "epoch: 26 loss: 0.4024341106414795\n",
            "epoch: 27 loss: 0.4012221693992615\n",
            "epoch: 28 loss: 0.4000319540500641\n",
            "epoch: 29 loss: 0.39886295795440674\n",
            "epoch: 30 loss: 0.3977145850658417\n",
            "epoch: 31 loss: 0.3965865969657898\n",
            "epoch: 32 loss: 0.39547839760780334\n",
            "epoch: 33 loss: 0.39438965916633606\n",
            "epoch: 34 loss: 0.3933198153972626\n",
            "epoch: 35 loss: 0.3922685384750366\n",
            "epoch: 36 loss: 0.3912354111671448\n",
            "epoch: 37 loss: 0.3902200162410736\n",
            "epoch: 38 loss: 0.38922202587127686\n",
            "epoch: 39 loss: 0.3882409334182739\n",
            "epoch: 40 loss: 0.38727647066116333\n",
            "epoch: 41 loss: 0.3863282799720764\n",
            "epoch: 42 loss: 0.3853960335254669\n",
            "epoch: 43 loss: 0.3844793140888214\n",
            "epoch: 44 loss: 0.38357776403427124\n",
            "epoch: 45 loss: 0.3826912045478821\n",
            "epoch: 46 loss: 0.38181909918785095\n",
            "epoch: 47 loss: 0.3809613585472107\n",
            "epoch: 48 loss: 0.38011759519577026\n",
            "epoch: 49 loss: 0.3792875111103058\n",
            "epoch: 50 loss: 0.3784708082675934\n",
            "epoch: 51 loss: 0.3776671886444092\n",
            "epoch: 52 loss: 0.3768764138221741\n",
            "epoch: 53 loss: 0.37609824538230896\n",
            "epoch: 54 loss: 0.3753323256969452\n",
            "epoch: 55 loss: 0.3745785355567932\n",
            "epoch: 56 loss: 0.3738364577293396\n",
            "epoch: 57 loss: 0.3731059730052948\n",
            "epoch: 58 loss: 0.3723868727684021\n",
            "epoch: 59 loss: 0.3716788589954376\n",
            "epoch: 60 loss: 0.3709816634654999\n",
            "epoch: 61 loss: 0.37029507756233215\n",
            "epoch: 62 loss: 0.36961907148361206\n",
            "epoch: 63 loss: 0.36895349621772766\n",
            "epoch: 64 loss: 0.3682979643344879\n",
            "epoch: 65 loss: 0.3676522672176361\n",
            "epoch: 66 loss: 0.3670162260532379\n",
            "epoch: 67 loss: 0.36638960242271423\n",
            "epoch: 68 loss: 0.36577218770980835\n",
            "epoch: 69 loss: 0.3651639223098755\n",
            "epoch: 70 loss: 0.3645645081996918\n",
            "epoch: 71 loss: 0.3639737665653229\n",
            "epoch: 72 loss: 0.363391637802124\n",
            "epoch: 73 loss: 0.36281782388687134\n",
            "epoch: 74 loss: 0.36225229501724243\n",
            "epoch: 75 loss: 0.3616948127746582\n",
            "epoch: 76 loss: 0.3611452579498291\n",
            "epoch: 77 loss: 0.3606034815311432\n",
            "epoch: 78 loss: 0.3600693345069885\n",
            "epoch: 79 loss: 0.3595426082611084\n",
            "epoch: 80 loss: 0.35902321338653564\n",
            "epoch: 81 loss: 0.3585110306739807\n",
            "epoch: 82 loss: 0.35800594091415405\n",
            "epoch: 83 loss: 0.3575078547000885\n",
            "epoch: 84 loss: 0.3570164442062378\n",
            "epoch: 85 loss: 0.3565317988395691\n",
            "epoch: 86 loss: 0.3560536503791809\n",
            "epoch: 87 loss: 0.35558202862739563\n",
            "epoch: 88 loss: 0.355116605758667\n",
            "epoch: 89 loss: 0.35465744137763977\n",
            "epoch: 90 loss: 0.35420435667037964\n",
            "epoch: 91 loss: 0.3537573516368866\n",
            "epoch: 92 loss: 0.35331615805625916\n",
            "epoch: 93 loss: 0.3528807759284973\n",
            "epoch: 94 loss: 0.35245102643966675\n",
            "epoch: 95 loss: 0.3520267903804779\n",
            "epoch: 96 loss: 0.35160812735557556\n",
            "epoch: 97 loss: 0.351194828748703\n",
            "epoch: 98 loss: 0.35078680515289307\n",
            "epoch: 99 loss: 0.3503839373588562\n",
            "epoch: 100 loss: 0.3499862551689148\n",
            "epoch: 101 loss: 0.34959349036216736\n",
            "epoch: 102 loss: 0.34920576214790344\n",
            "epoch: 103 loss: 0.34882280230522156\n",
            "epoch: 104 loss: 0.3484446406364441\n",
            "epoch: 105 loss: 0.3480711877346039\n",
            "epoch: 106 loss: 0.347702294588089\n",
            "epoch: 107 loss: 0.34733790159225464\n",
            "epoch: 108 loss: 0.34697800874710083\n",
            "epoch: 109 loss: 0.346622496843338\n",
            "epoch: 110 loss: 0.3462713062763214\n",
            "epoch: 111 loss: 0.3459243178367615\n",
            "epoch: 112 loss: 0.3455814719200134\n",
            "epoch: 113 loss: 0.3452427387237549\n",
            "epoch: 114 loss: 0.34490805864334106\n",
            "epoch: 115 loss: 0.34457728266716003\n",
            "epoch: 116 loss: 0.3442504107952118\n",
            "epoch: 117 loss: 0.34392738342285156\n",
            "epoch: 118 loss: 0.34360820055007935\n",
            "epoch: 119 loss: 0.3432926833629608\n",
            "epoch: 120 loss: 0.3429808020591736\n",
            "epoch: 121 loss: 0.3426724970340729\n",
            "epoch: 122 loss: 0.34236782789230347\n",
            "epoch: 123 loss: 0.34206655621528625\n",
            "epoch: 124 loss: 0.34176868200302124\n",
            "epoch: 125 loss: 0.3414742648601532\n",
            "epoch: 126 loss: 0.3411831259727478\n",
            "epoch: 127 loss: 0.34089523553848267\n",
            "epoch: 128 loss: 0.34061065316200256\n",
            "epoch: 129 loss: 0.34032920002937317\n",
            "epoch: 130 loss: 0.3400508463382721\n",
            "epoch: 131 loss: 0.33977559208869934\n",
            "epoch: 132 loss: 0.33950328826904297\n",
            "epoch: 133 loss: 0.33923405408859253\n",
            "epoch: 134 loss: 0.3389677405357361\n",
            "epoch: 135 loss: 0.33870428800582886\n",
            "epoch: 136 loss: 0.33844369649887085\n",
            "epoch: 137 loss: 0.3381858766078949\n",
            "epoch: 138 loss: 0.3379308879375458\n",
            "epoch: 139 loss: 0.33767858147621155\n",
            "epoch: 140 loss: 0.3374289870262146\n",
            "epoch: 141 loss: 0.33718201518058777\n",
            "epoch: 142 loss: 0.33693763613700867\n",
            "epoch: 143 loss: 0.3366958498954773\n",
            "epoch: 144 loss: 0.3364565968513489\n",
            "epoch: 145 loss: 0.33621981739997864\n",
            "epoch: 146 loss: 0.3359854221343994\n",
            "epoch: 147 loss: 0.33575358986854553\n",
            "epoch: 148 loss: 0.3355240523815155\n",
            "epoch: 149 loss: 0.3352968990802765\n",
            "epoch: 150 loss: 0.3350720703601837\n",
            "epoch: 151 loss: 0.3348495066165924\n",
            "epoch: 152 loss: 0.3346291780471802\n",
            "epoch: 153 loss: 0.3344111144542694\n",
            "epoch: 154 loss: 0.33419519662857056\n",
            "epoch: 155 loss: 0.333981454372406\n",
            "epoch: 156 loss: 0.33376988768577576\n",
            "epoch: 157 loss: 0.33356037735939026\n",
            "epoch: 158 loss: 0.3333529829978943\n",
            "epoch: 159 loss: 0.3331475555896759\n",
            "epoch: 160 loss: 0.33294421434402466\n",
            "epoch: 161 loss: 0.3327428102493286\n",
            "epoch: 162 loss: 0.33254337310791016\n",
            "epoch: 163 loss: 0.3323459029197693\n",
            "epoch: 164 loss: 0.3321503698825836\n",
            "epoch: 165 loss: 0.33195656538009644\n",
            "epoch: 166 loss: 0.33176475763320923\n",
            "epoch: 167 loss: 0.3315747380256653\n",
            "epoch: 168 loss: 0.33138659596443176\n",
            "epoch: 169 loss: 0.3312002122402191\n",
            "epoch: 170 loss: 0.33101552724838257\n",
            "epoch: 171 loss: 0.3308326005935669\n",
            "epoch: 172 loss: 0.33065134286880493\n",
            "epoch: 173 loss: 0.33047184348106384\n",
            "epoch: 174 loss: 0.3302939832210541\n",
            "epoch: 175 loss: 0.33011776208877563\n",
            "epoch: 176 loss: 0.3299432098865509\n",
            "epoch: 177 loss: 0.32977020740509033\n",
            "epoch: 178 loss: 0.3295988440513611\n",
            "epoch: 179 loss: 0.3294290006160736\n",
            "epoch: 180 loss: 0.3292607367038727\n",
            "epoch: 181 loss: 0.3290940225124359\n",
            "epoch: 182 loss: 0.32892879843711853\n",
            "epoch: 183 loss: 0.32876497507095337\n",
            "epoch: 184 loss: 0.32860273122787476\n",
            "epoch: 185 loss: 0.328441858291626\n",
            "epoch: 186 loss: 0.3282824456691742\n",
            "epoch: 187 loss: 0.328124463558197\n",
            "epoch: 188 loss: 0.3279678523540497\n",
            "epoch: 189 loss: 0.32781264185905457\n",
            "epoch: 190 loss: 0.3276587724685669\n",
            "epoch: 191 loss: 0.32750624418258667\n",
            "epoch: 192 loss: 0.3273550570011139\n",
            "epoch: 193 loss: 0.32720518112182617\n",
            "epoch: 194 loss: 0.3270566165447235\n",
            "epoch: 195 loss: 0.32690930366516113\n",
            "epoch: 196 loss: 0.32676324248313904\n",
            "epoch: 197 loss: 0.326618492603302\n",
            "epoch: 198 loss: 0.3264748454093933\n",
            "epoch: 199 loss: 0.32633256912231445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJtIQnDS1iLk"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}